# PVZ's Event Summary Exporter for Airlock
# Patrick Van Zandt
#
# This is an example of how to bulk download Airlock Execution History data (events
# generated by Airlock Enforcement Agents) from Airlock Server using the Airlock Digital REST API
# and then analyze them using the Pandas Data Analysis Library for Python, specifically to identify 
# and report on the most common activity patterns. The results can be useful for understanding large 
# data sets, especially when focused on simulated blocks ("Untrusted Execution [Audit]") events during
# initial deployment while you are running Airlock in Audit Mode and identifying candidates to consider 
# adding to your allowlist in the interest of reducing overall event volume and preparing to transition
# to Enforcement Mode.
#
# This script requires an API key for a user in a group with the following API role
# permissions:
#
#     logging/exechistories
#
# This script ingests configuration from airlock.yaml, which you must create and place in the same directory
# as this .py file. Use any text editor of your choice and follow the template below.
'''

server_name: x.y.managedwhitelisting.com
api_key: z

event_summary_exporter:

  event_types:
   #- 0  #Trusted Execution 
   #- 1  #Blocked Execution 
   - 2  #Untrusted Execution [Audit] 
   #- 3  #Untrusted Execution [OTP] 
   #- 4  #Trusted Path Execution 
   #- 5  #Trusted Publisher Execution 
   #- 6  #Blocklist Execution 
   #- 7  #Blocklist Execution [Audit] 
   #- 8  #Trusted Process Execution

  lookback_hours: 168

  max_event_quantity: 10000000

  top_n_values: 100

  policy_groups:
    - Policy Group Name 1 
    - Policy Group Name 2
    - Policy Group Name 3

''' 
# For more details on the required and optional fields and syntax, reference the documentation 
# in event_summary_exporter.md
#
# This script requires Python 3.x and several libraries. To install the libraries run this command:
#    pip install requests pyyaml pandas openpyxl pymongo


## Import required libraries ##
import os, re, json, sys
from datetime import datetime, timedelta, timezone
import requests
import yaml
import pandas
from bson.objectid import ObjectId
from openpyxl.utils import get_column_letter


## READ AND PROCESS CONFIGURATION ##

# Load configuration from YAML
config_file_path = 'airlock.yaml'
if not os.path.exists(config_file_path):
    print('ERROR: Configuration file', config_file_path, 'does not exist')
    sys.exit(1)
print('Reading configuration from', config_file_path)
with open(config_file_path, 'r') as file:
    config = yaml.safe_load(file)

# Extract configuration parameters, using default values for any missing parameters
print('Processing configuration')
server_name = config['server_name']
print(' ', 'server_name:', server_name)
api_key = config['api_key']
print(' ', 'api_key:', f'{'*' * (len(api_key) - 4)}{api_key[-4:]}')
lookback_hours = config['event_summary_exporter'].get('lookback_hours', 168) #default is 1 week (24*7=168)
print(' ', 'lookback_hours:', lookback_hours)
event_types = config['event_summary_exporter'].get('event_types', [2]) #default is Untrusted Execution [Audit] (2)
print(' ', 'event_types:', event_types)
max_event_quantity = config['event_summary_exporter'].get('max_event_quantity', 10000000) #default is 10M
print(' ', 'max_event_quantity:', max_event_quantity)
top_n_values = config['event_summary_exporter'].get('top_n_values', 25) #default is 25
print(' ', 'top_n_values:', top_n_values)
policy_groups = config['event_summary_exporter'].get('policy_groups', []) #default is no filter (all Policy Groups)
print(' ', 'policy_groups:', policy_groups)


## PREPARE TO DOWNLOAD EVENTS ##
    
# Calculate database checkpoint and human-readable strings regarding window of time to gather events for
end_time = datetime.now(timezone.utc)
end_time_str = end_time.strftime('%Y-%m-%d_%H-%M_UTC')
start_time = end_time - timedelta(hours=lookback_hours)
start_time_str = start_time.strftime('%Y-%m-%d_%H-%M_UTC')
checkpoint = str(ObjectId.from_datetime(start_time))

# Print timestamps, checkpoint, and summary of download action before beginning
print('Current time is', end_time_str)
print('Minimum checkpoint for events ingested at', start_time_str, f"({lookback_hours} hours ago)", 'is', checkpoint)
print(f"Downloading up to {max_event_quantity:,} events from Airlock Server of type(s) {event_types} received at or after {start_time_str}")

# Define parameters for making requests to server
request_url = f'https://{server_name}:3129/v1/logging/exechistories'
request_headers = {'X-ApiKey': api_key}
request_body = {'type': event_types, 
                'policy': policy_groups, 
                'checkpoint': checkpoint}

# Define a list to store events as they are downloaded
events = []

# Define a counter to keep track of how many batches (pages) of events have been downloaded
batch_counter = 0


## DOWNLOAD EVENTS FROM AIRLOCK SERVER ##

# Repeat this block of code until a break condition is identified
while True:
   
    # If maximum event quantity has been reached, exit the while loop
    if len(events) >= max_event_quantity:
        print('Maximum event quantity has been reached, stopping download')
        break
    
    # Get a batch of events from server and increment batch counter
    response = requests.post(request_url, headers=request_headers, json=request_body)
    events_this_batch = response.json()['response']['exechistories']
    batch_counter += 1

    # If no events were returned, exit the while loop
    if events_this_batch is None:
        print('No events returned, indicating download is complete')
        break
    
    # Add this batch of events to the collected events
    events += events_this_batch

    # Print progress to console
    print(
        'Request for events with checkpoint >', request_body['checkpoint'], 
        'returned', '{:,}'.format(len(events_this_batch)), 'events',
        'and total events downloaded is now', '{:,}'.format(len(events)),
        'after', batch_counter, 'requests to server.',
        'Last event was ingested at', ObjectId(events_this_batch[len(events_this_batch)-1]['checkpoint']).generation_time, '.'
        )

    # If less than 10,000 events were returned, exit the while loop
    if len(events_this_batch) < 10000:
        print('Less than 10,000 events returned, indicating download is complete')
        break
    
    else:
        # 10,000 events were returned on the last batch, indicative there may be more. Continue download loop.

        # Extract checkpoint from the last event returned
        last_event_checkpoint = events_this_batch[len(events_this_batch)-1]['checkpoint']
        
        # Modify request body to pass the last downloaded event's checkpoint for next request to server
        request_body['checkpoint'] = last_event_checkpoint

# Print summary of the event download process
print('Event download is complete. Downloaded', '{:,}'.format(len(events)), 'events in', batch_counter, 'batches')

# If no events were downloaded, exit script
if len(events) == 0:
    print('No events were found')
    sys.exit(1)


## PRE-PROCESS THE DOWNLOADED EVENTS ##

print('Loading events into a Pandas DataFrame')
events_df = pandas.DataFrame(events)

# Generalize usernames so that grouping can aggregate paths and processes across multiple users
print('Replacing usernames with asterisks')
for column in ['filename', 'pprocess']:
    events_df[column] = events_df[column].str.replace(r'C:\\users\\[^\\]+', r'C:\\users\\*', regex=True, flags=re.IGNORECASE)
    events_df[column] = events_df[column].str.replace('/Users/[^/]+', '/Users/*', regex=True, flags=re.IGNORECASE)   

# Rename columns for easier readability in output file
column_rename_mapping = {
                        'filename': 'filename_full',
                        'pprocess': 'parent_process_full',
                        'sha256': 'file_hash'
                       }
print('Renaming columns with mapping', column_rename_mapping)
for key in column_rename_mapping.keys():
    print(' ', key, '-->', column_rename_mapping[key])
events_df.rename(columns=column_rename_mapping, inplace=True)
                   
# Split 'filename_full' column into 'folder' and 'file' columns
print('Splitting filename_full column into folder and file columns')
events_df[['folder', 'file']] = events_df['filename_full'].str.rpartition('\\').iloc[:, [0, 2]]
events_df['folder'] = events_df['folder'] + '\\'

# Split 'parent_process_full' column to create 'parent_process_name'
print('Extracting parent_process_name from parent_process_full')
events_df['parent_process_name'] = events_df['parent_process_full'].str.rpartition('\\')[2]

# Define list of columns to keep (also used to determine sheet order in exported Excel file)
export_columns = ['file_hash', 'folder', 'file', 'filename_full', 'parent_process_name', 'parent_process_full', 'publisher', 'hostname']
print('Dropping all columns except', export_columns)
events_df = events_df.loc[:, export_columns]


## ANALYZE THE DATA TO FIND TOP VALUES ##

results = {}
print('Analyzing data to find the', top_n_values, 'most common values for each of the remaining', len(export_columns) ,'columns')
for field in export_columns:
    print(' ', field, end=' ')
    counts = events_df[field].value_counts().head(top_n_values)
    total = len(events_df)
    percentages = (counts / total * 100).round(2)
    results[field] = [
        {field: value, "Count": count, "Percentage": percent}
        for value, count, percent in zip(counts.index, counts, percentages)
        ]
    print('[Done]')
print('Analysis is complete')


## WRITE THE RESULTS TO DISK ##

print('Beginning export of results')

output_file_name = server_name.split(".")[0] + '_event_summary_' + start_time_str + '_to_' + end_time_str + '_' + str(len(events)) + '.xlsx'
print('Data will be written to', output_file_name)

with pandas.ExcelWriter(output_file_name, engine='openpyxl') as writer:
    for sheet_name, data in results.items():
        sheet_df = pandas.DataFrame(data)
        sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)
        worksheet = writer.sheets[sheet_name]
        for col_idx, col_name in enumerate(sheet_df.columns, 1):
            max_length = max(sheet_df[col_name].astype(str).map(len).max(), len(col_name)) + 2
            column_letter = get_column_letter(col_idx)
            worksheet.column_dimensions[column_letter].width = max_length
            if col_name == "Percentage":
                for cell in worksheet[column_letter]:
                    if isinstance(cell.value, (int, float)):
                        cell.number_format = '0.00%'
                        cell.value = cell.value / 100

print('Export is done')
