readme_message = '''
Welcome to PVZ's Event Summary Exporter for Airlock Digital
Created by Patrick Van Zandt, published under GNU GPL 3.0, intended as a working example. No warranty or support.

This is an open-source example of how to bulk download Airlock Execution History data (events generated by Airlock Enforcement Agents) from Airlock Server using the Airlock Digital REST API and then analyze them using the Pandas Data Analysis Library for Python, specifically to identify and report on the most common activity patterns. 

This analysis can be helpful for understanding large data sets, especially when focused on simulated blocks ("Untrusted Execution [Audit]") events during initial deployment while you are running Airlock in Audit Mode and identifying candidates to consider adding to your allowlist in the interest of reducing overall event volume and preparing to transition to Enforcement Mode.

This script requires Python 3.x and several libraries. To install the libraries run this command:
    pip install requests pyyaml pandas openpyxl pymongo

This script requires an Airlock API key for a user in a group with the following API role permissions:
     logging/exechistories

This script ingests configuration from airlock.yaml, which you must create and place in the same directory as this .py file. Use any text editor of your choice and follow the template below.

server_name: foo.bar.managedwhitelisting.com
api_key: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa

There are also several optional parameters, for example you can target a specific Policy Group with an airlock.yaml file that looks like this:

server_name: foo.bar.managedwhitelisting.com
api_key: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
event_summary_exporter:
  policy_groups:
    - My Workstations Audit Group 1

For full details reference the documentation in event_summary_exporter.md.
'''

print(readme_message)

## Import required libraries ##
import os, re, json, sys
from datetime import datetime, timedelta, timezone
import requests
import yaml
import pandas
from bson.objectid import ObjectId
from openpyxl.utils import get_column_letter


## READ AND PROCESS CONFIGURATION ##

# Load configuration from YAML
config_file_path = 'airlock.yaml'
if not os.path.exists(config_file_path):
    print('ERROR: Configuration file', config_file_path, 'does not exist')
    sys.exit(1)
print('Reading configuration from', config_file_path)
with open(config_file_path, 'r') as file:
    config = yaml.safe_load(file)

# Extract configuration parameters, using default values for any missing parameters
print('Processing configuration')
server_name = config['server_name']
print(' ', 'server_name:', server_name)
api_key = config['api_key']
print(' ', 'api_key:', f"{'*' * (len(api_key) - 4)}{api_key[-4:]}")
lookback_hours = config['event_summary_exporter'].get('lookback_hours', 168) #default is 1 week (24*7=168)
print(' ', 'lookback_hours:', lookback_hours)
event_types = config['event_summary_exporter'].get('event_types', [2]) #default is Untrusted Execution [Audit] (2)
print(' ', 'event_types:', event_types)
max_event_quantity = config['event_summary_exporter'].get('max_event_quantity', 10000000) #default is 10 million
print(' ', 'max_event_quantity:', max_event_quantity)
top_n_values = config['event_summary_exporter'].get('top_n_values', 100) #default is 100
print(' ', 'top_n_values:', top_n_values)
policy_groups = config['event_summary_exporter'].get('policy_groups', []) #default all Policy Groups
print(' ', 'policy_groups:', policy_groups)


## PREPARE TO DOWNLOAD EVENTS ##
    
# Calculate database checkpoint and human-readable strings regarding window of time to gather events for
now = datetime.now(timezone.utc)
now_str = now.strftime('%Y-%m-%d_%H-%M_UTC')
start_time = now - timedelta(hours=lookback_hours)
start_time_str = start_time.strftime('%Y-%m-%d_%H-%M_UTC')
checkpoint = str(ObjectId.from_datetime(start_time))

# Print timestamps, checkpoint, and summary of download action before beginning
print('Current time is', now_str)
print('Minimum checkpoint for events ingested at', start_time_str, f"({lookback_hours} hours ago)", 'is', checkpoint)
print(f"Downloading up to {max_event_quantity:,} events from Airlock Server of type(s) {event_types} received at or after {start_time_str}")

# Define parameters for making requests to server
request_url = f'https://{server_name}:3129/v1/logging/exechistories'
request_headers = {'X-ApiKey': api_key}
request_body = {'type': event_types, 
                'policy': policy_groups, 
                'checkpoint': checkpoint}

# Define a list to store events as they are downloaded
events = []

# Define a counter to keep track of how many batches (pages) of events have been downloaded
batch_counter = 0


## DOWNLOAD EVENTS FROM AIRLOCK SERVER ##

# Repeat this block of code until a break condition is identified
while True:
   
    # If maximum event quantity has been reached, exit the while loop
    if len(events) >= max_event_quantity:
        print('Maximum event quantity has been reached, stopping download')
        break
    
    # Get a batch of events from server and increment batch counter
    response = requests.post(request_url, headers=request_headers, json=request_body)
    events_this_batch = response.json()['response']['exechistories']
    batch_counter += 1

    # If no events were returned, exit the while loop
    if events_this_batch is None:
        print('No events returned, indicating download is complete')
        break
    
    # Add this batch of events to the collected events
    events += events_this_batch

    # Extract checkpoint and ingestion timestamp from last event in this batch
    last_event = events_this_batch[-1]
    last_checkpoint = last_event['checkpoint']  # hex string
    last_ingest_dt_utc = ObjectId(last_checkpoint).generation_time  #ObjectId gen time is UTC
    last_ingest_str_utc = last_ingest_dt_utc.strftime('%Y-%m-%d_%H-%M_UTC')

    # Print progress to console
    print(
        'Request', batch_counter, 'for events with checkpoint >', request_body['checkpoint'], 
        'returned', '{:,}'.format(len(events_this_batch)), 'events',
        'with the last event being ingested by the server at', last_ingest_str_utc,
        'and total events downloaded is now', '{:,}'.format(len(events))
        )

    # If less than 10,000 events were returned, exit the while loop
    if len(events_this_batch) < 10000:
        print('Less than 10,000 events returned, indicating download is complete')
        break
    
    else:
        # 10,000 events were returned on the last batch, indicative there may be more
        request_body['checkpoint'] = last_checkpoint  #increment checkpoint before continuing download

# Print summary of the event download process
print('Downloaded', '{:,}'.format(len(events)), 'events in', batch_counter, 'batches')

# If no events were downloaded, exit script
if len(events) == 0:
    print('No events were found')
    sys.exit(1)


## PRE-PROCESS THE DOWNLOADED EVENTS ##

print('Loading events into a Pandas DataFrame')
events_df = pandas.DataFrame(events)

# List of columns containing paths
path_columns = ['filename', 'pprocess']

# Convert all Windows paths to lowercase only -- useful for grouping purposes since we want c:\folder\filename.exe and c:\Folder\Filename.exe to be combined when calculating top values
print('Lowercasing Windows paths')
win_mask = events_df[path_columns].apply(lambda s: s.str.contains(r'([A-Za-z]:\\|\\\\)', na=False)).any(axis=1)
events_df.loc[win_mask, path_columns] = events_df.loc[win_mask, path_columns].apply(lambda s: s.str.lower())

# Generalize usernames - useful because we want c:\users\user01\foo\bar.exe and c:\users\user02\foo\bar.exe to be combined when calculating top values
print('Removing usernames and replacing with *')
for col in path_columns:
    events_df[col] = events_df[col].str.replace(r'C:\\users\\[^\\]+', r'C:\\users\\*', regex=True, flags=re.IGNORECASE)
    events_df[col] = events_df[col].str.replace(r'/Users/[^/]+', '/Users/*', regex=True, flags=re.IGNORECASE)

# Rename columns for easier readability in output file
column_rename_mapping = {
                        'filename': 'filename_full',
                        'pprocess': 'parent_process_full',
                        'sha256': 'file_hash'
                       }
print('Renaming columns with mapping', column_rename_mapping)
events_df.rename(columns=column_rename_mapping, inplace=True)
                   
# Split 'filename_full' column into 'folder' and 'file' columns
print('Splitting filename_full column into folder and file columns')
events_df[['folder', 'file']] = events_df['filename_full'].str.rpartition('\\').iloc[:, [0, 2]]
events_df['folder'] = events_df['folder'] + '\\'

# Split 'parent_process_full' column to create 'parent_process_name'
print('Extracting parent_process_name from parent_process_full')
events_df['parent_process_name'] = events_df['parent_process_full'].str.rpartition('\\')[2]

# Define list of columns to keep (also used to determine sheet order in exported Excel file)
export_columns = ['file_hash', 'folder', 'file', 'filename_full', 'parent_process_name', 'parent_process_full', 'publisher', 'hostname']
print('Dropping all columns except', export_columns)
events_df = events_df.loc[:, export_columns]


## ANALYZE THE DATA TO FIND TOP VALUES AND THEIR METRICS ##

results = {}
print(f"Beginning data analysis to calculate most common values and associated metrics for each of the remaining {len(export_columns)} columns")

# Iterate through each of the defined export columns, performing below steps on each
counter = 1
for field in export_columns:

    print(f"Analyzing field {counter}/{len(export_columns)}: {field}")
    
    # Step 1: Group the dataset by the current field
    # For each unique value of `field`:
    #   - Count = how many total events had this value
    #   - UniqueHostnames = how many distinct hostnames generated those events
    grouped_stats = events_df.groupby(field).agg(
        Count=(field, 'size'),                    # total rows for this value
        UniqueHostnames=('hostname', 'nunique')   # distinct hostnames for this value
    )

    # Step 2: Sort by Count and keep only the top N values
    top_values = grouped_stats.nlargest(top_n_values, 'Count')

    # Step 3: Compute percentage of total events for each of the top values
    total_events = len(events_df)
    percentages = (top_values['Count'] / total_events * 100).round(2)

    # Step 4: Build result rows (convert to list of dicts for export)
    results[field] = [
        {
            field: value,                              # the actual value from this column
            "Count": int(row.Count),                   # how many events had this value
            "Percentage": float(percent),              # % of all events this represents
            "Unique Hostnames": int(row.UniqueHostnames) # how many distinct hosts saw it
        }
        for (value, row), percent in zip(top_values.iterrows(), percentages)
    ]

    print('Done')
    counter += 1

print('Data analysis is complete')



## WRITE THE RESULTS TO DISK ##

print('Beginning export of results')

server_alias = server_name.split('.')[0]
output_file_name = f"{server_alias}_airlock_event_summary_{start_time_str}_to_{last_ingest_str_utc}_{str(len(events))}.xlsx"

print('Data will be written to', output_file_name)

with pandas.ExcelWriter(output_file_name, engine='openpyxl') as writer:
    for sheet_name, data in results.items():
        sheet_df = pandas.DataFrame(data)
        sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)
        worksheet = writer.sheets[sheet_name]
        for col_idx, col_name in enumerate(sheet_df.columns, 1):
            max_length = max(sheet_df[col_name].astype(str).map(len).max(), len(col_name)) + 2
            column_letter = get_column_letter(col_idx)
            worksheet.column_dimensions[column_letter].width = max_length
            if col_name == "Percentage":
                for cell in worksheet[column_letter]:
                    if isinstance(cell.value, (int, float)):
                        cell.number_format = '0.00%'
                        cell.value = cell.value / 100

print('Export is done')
